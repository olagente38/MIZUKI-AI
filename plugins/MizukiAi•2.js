import axios from 'axios'
import fetch from 'node-fetch'
import * as tf from '@tensorflow/tfjs'
import * as mobilenet from '@tensorflow-models/mobilenet'

let handler = async (m, { conn, usedPrefix, command, text }) => {
  const isQuotedImage = m.quoted && (m.quoted.msg || m.quoted).mimetype && (m.quoted.msg || m.quoted).mimetype.startsWith('image/')
  const username = `${conn.getName(m.sender)}`
  const basePrompt = `Tu nombre es Mizuki AI y parece haber sido creado por SpectrumOfc. Tu versiÃ³n actual es 2.0.7, TÃº usas el idioma EspaÃ±ol. LlamarÃ¡s a las personas por su nombre ${username}, te gusta ser divertida, y te encanta aprender. Lo mÃ¡s importante es que debes ser amigable con la persona con la que estÃ¡s hablando. ${username}`

  if (isQuotedImage) {
    const q = m.quoted
    const img = await q.download?.()
    if (!img) {
      console.error('[ðŸš¨] ð„ð«ð«ð¨ð«: ðð¨ ð¢ð¦ðšð ðž ð›ð®ðŸðŸðžð« ðšð¯ðšð¢ð¥ðšð›ð¥ðž.')
      return conn.reply(m.chat, '*[ðŸš¨] ð’ð¨ð¥ð¢ðœð¢ð­ð®ð ð¢ð§ðœð¨ð¦ð©ð¥ðžð­ðš. ðŒð¢ð³ð®ð¤ð¢ ð€ðˆ ð§ð¨ ð©ð®ðžððž ð«ðžð¬ð©ð¨ð§ððžð« ðš ð¢ð¦Ã¡ð ðžð§ðžð¬.*', m, fake)
    }

    try {
      const model = await mobilenet.load()
      const tensor = tf.node.decodeImage(img)
      const predictions = await model.classify(tensor)
      const description = predictions.map(p => `${p.className}: ${p.probability.toFixed(2)}`).join('\n')
      
      const responsePrompt = `${basePrompt}. La imagen que se analiza contiene: ${description}`
      const response = await luminsesi('[ðŸš¨] *DescrÃ­beme la imagen y detalla por quÃ© actÃºan asÃ­. TambiÃ©n dime quiÃ©n eres*', username, responsePrompt)

      await conn.reply(m.chat, response, m, fake)
    } catch (error) {
      console.error('Error:', error)
      await conn.reply(m.chat, '[ðŸš¨] Solicitud incompleta. Mizuki AI no pudo analizar la imagen.', m, fake)
    }
  } else {
    if (!text) {
      return conn.reply(m.chat, `[ðŸš¨] Solicitud incompleta. Intente nuevamente, esta vez proporcionando un mensaje de consulta.\n\n[âœ…] Ejemplo: */${command}* *Â¿CÃ³mo estÃ¡s hoy?*`, m)
    }

    await m.react(rwait)
    try {
      const { key } = await conn.sendMessage(m.chat, {text: `[ðŸš¨] Solicitud en proceso. Mizuki AI estÃ¡ procesando tu peticiÃ³n, espera unos segundos.`}, {quoted: m})
      const query = text
      const prompt = `${basePrompt}. Responde lo siguiente: ${query}`
      const response = await luminsesi(query, username, prompt)
      await conn.sendMessage(m.chat, {text: response, edit: key})
      await m.react(done)
    } catch {
      await m.react(error)
      await conn.reply(m.chat, '[ðŸš¨] Mizuki AI no puede responder a esa pregunta.', m, fake)
    }
  }
}

handler.help = ['ai2', 'chatgpt2']
handler.tags = ['ia2']
handler.register = true
handler.command = ['ai2', 'ia2', 'chatgpt2']

export default handler

const delay = (ms) => new Promise((resolve) => setTimeout(resolve, ms))

// FunciÃ³n para interactuar con la IA usando prompts
async function luminsesi(q, username, logic) {
  try {
    const response = await axios.post("https://Luminai.my.id", {
      content: q,
      user: username,
      prompt: logic,
      webSearchMode: false
    })
    return response.data.result
  } catch (error) {
    console.error('âœ˜ Error al obtener:', error)
    throw error
  }
}
